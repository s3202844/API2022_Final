<!DOCTYPE html>
<html>

<link href="style.css" rel="stylesheet" type="text/css" />

<body>
    <div class="container">

        <head>
            <meta charset="utf-8">
            <title>API2022 Final</title>
            <link rel="shortcut icon" href="data/leidenuniv.ico">
        </head>

        <main class="main">
            <h1 class="title">Sheet Music Generation</h1>
            <!-- <h3 class="authorby">Fall 2022 API Final Project by:</h3> -->
            <center>
                <h3 class="">Yuang Yuan, Haoran Yin, Pim Bax</h3>
            </center>
            <!-- <div class="abstract">
                <div class="section">
                    <h1 class="sectiontitle">Abstract</h1>
                </div>
            </div> -->

            <div class="section">
                <h1 class="sectiontitle">Audio Separation</h1>
                <p class="paragraph">
                    Audio separation is the first step of Sheet Music Generation in which the raw audio will be
                    separated into different tracks including vocal, paino, guitar and so on. We can then use the
                    separated audios to generate MIDI files.</p>
                <p class="paragraph">
                    Our audio source separation method is based on Demucs<a href="#cite4">[1]</a>, a hybrid
                    transform-based model, proposed by Facebook AI Research. See below the example audio separation
                    results:</p>

                <p>
                <div class="audiopreview">
                    <div>
                        <p class="paragraph">Raw Audio (from MUSDB18):</p>
                        <audio controls src="data/raw.mp3">
                            Your browser does not support the
                            <code>audio</code> element.
                        </audio>
                    </div>
                </div>
                <div class="audiopreview">
                    <div>
                        <p class="paragraph">Separated Track(vocals):</p>
                        <audio controls src="data/vocals_sep.mp3">
                            Your browser does not support the
                            <code>audio</code> element.
                        </audio>
                    </div>
                    <div>
                        <p class="paragraph">Separated Track(drums):</p>
                        <audio controls src="data/drums.mp3">
                            Your browser does not support the
                            <code>audio</code> element.
                        </audio>
                    </div>
                </div>
                <div class="audiopreview">
                    <div>
                        <p class="paragraph">Separated Track(bass):</p>
                        <audio controls src="data/bass.mp3">
                            Your browser does not support the
                            <code>audio</code> element.
                        </audio>
                    </div>
                    <div>
                        <p class="paragraph">Separated Track(other):</p>
                        <audio controls src="data/other.mp3">
                            Your browser does not support the
                            <code>audio</code> element.
                        </audio>
                    </div>
                </div>
                </p>
            </div>


            <div class="section">
                <h1 class="sectiontitle">Audio to MIDI</h1>

                <p class="paragraph">
                    MIDI (Musical Instrument Digital Interface) was proposed in the early 1980s to solve the
                    communication problem between electro-acoustic instruments. MIDI is the most extensive music
                    standard format in the arranger world, and it can be called "music notation that computers can
                    understand." It uses digital control signals of notes to record music <a href="#cite1">[2]</a>.
                </p>
                <p>
                    Considering the importance of this part, we chose a novel research,
                    <a href="https://basicpitch.spotify.com/" target="_blank">Basic Pitch</a>, as the basis of our
                    work <a href="#cite2">[3]</a>. This research was released in conjunction with Spotify's publication
                    at ICASSP 2022. It introduces a lightweight yet powerful Automatic Music Transcription (AMT)
                    model. To train our model, we use three datasets,
                    <a href="https://www.upf.edu/web/mtg/mtg-qbh" target="_blank">MTG-QBH</a>,
                    <a href="https://magenta.tensorflow.org/datasets/maestro" target="_blank">MAESTRO</a>, and
                    <a href="https://source-separation.github.io/tutorial/data/musdb18.html"
                        target="_blank">MUSDB18</a>. Here are some exciting transcription results for different
                    instruments (including vocals).
                </p>

                <div class="audiopreview">
                    <div>
                        <p class="paragraph">Vocals (from MUSDB18):</p>
                        <audio controls src="data/vocals.mp3">
                            Your browser does not support the
                            <code>audio</code> element.
                        </audio>
                    </div>
                    <div>
                        <p class="paragraph">MIDI Transcription:</p>
                        <audio controls src="data/vocals_pre.mp3">
                            Your browser does not support the
                            <code>audio</code> element.
                        </audio>
                    </div>
                </div>
                <div class="audiopreview">
                    <div>
                        <p class="paragraph">
                            <a href="#sheet" id="piano">Piano</a> (from MTG-QBH):
                        </p>
                        <audio controls src="data/piano.mp3">
                            Your browser does not support the
                            <code>audio</code> element.
                        </audio>
                    </div>
                    <div>
                        <p class="paragraph">
                            <a href="#sheet_pre" id="midi_trans">MIDI Transcription:</a>
                        </p>
                        <audio controls src="data/piano_pre.mp3">
                            Your browser does not support the
                            <code>audio</code> element.
                        </audio>
                    </div>
                </div>
                <div class="audiopreview">
                    <div>
                        <p class="paragraph">Guitar (from MAESTRO):</p>
                        <audio controls src="data/guitar.mp3">
                            Your browser does not support the
                            <code>audio</code> element.
                        </audio>
                    </div>
                    <div>
                        <p class="paragraph">MIDI Transcription:</p>
                        <audio controls src="data/guitar_pre.mp3">
                            Your browser does not support the
                            <code>audio</code> element.
                        </audio>
                    </div>
                </div>

                <p></p>

                <div class="imagewrapper">
                    <img src="data/basic_pitch.png" width="90%" />
                </div>
                <p align="center"><b>Figure</b> , Comparison of <i>Basic Pitch</i> and a recent, strong baseline model,
                    MI-AMT <a href="#cite3">[4]</a>. It uses a U-Net architecture with an attention mechanism and
                    outputs a note-activation posteriorgram with a total of over 20M parameters, trained on MAESTRO and
                    MusicNet. Average note event metrics on all test datasets for the baseline algorithm, proposed
                    method, and ablation experiments. The best score for each column is in bold. The shade of green
                    indicates how far a score is from the best score, with the worst scores in white. All non-underlined
                    results are statistically significantly different with <i>p</i>
                    < 0.05 compared with NMP (permetric/dataset) via a paired t-test. </p>

            </div>

            <div class="section">
                <h1>MIDI to Note Sheet</h1>

                <p class="paragraph"> To generate the sheet music, we used the Mingus library in Python to read the midi
                    files.
                    This library is able to convert the midi files to an internal representation, which can then be
                    converted to
                    a so-called 'LilyPad' string. This string will then be used by LilyPad, another library, to generate
                    a PDF of
                    the sheet music.
                </p>
                <p class="paragraph">
                    When trying to read the midi files, we ran into some issues of Mingus not being able to interpret
                    the specific
                    midi format as generated in the earlier steps. This means that we first had to make some changes to
                    the midi output
                    of our previous steps, before we were able to interpret these midi files.
                </p>

                <p> Here are the sheets for 2 audio clips from last section, represents <i>Piano</i> and <i>MIDI
                        Transcription</i> respectively. </p>
                <p id="sheet">Sheet of ground truth(<a href="#piano">Piano</a>)</p>
                <div class="imagewrapper">
                    <img src="data/sheet.png" />
                </div>
                <p id="sheet_pre">Sheet of <a href="#midi_trans">MIDI transcription</a></p>
                <div class="imagewrapper">
                    <img src="data/sheet_pre.png" />
                </div>
            </div>



            <div class="section">
                <h1>Applications</h1>
                <h3>Play sheet</h3>
                <p>An Android application that plays sheet music. The video shows how it plays the sheet generated by
                    midi file from vocal sample above.</p>
                <div class="imagewrapper">
                    <video width="500" controls>
                        <source src="data/play_sheet.mp4" type="video/mp4">
                    </video>
                </div>
                <h3>Rhythm Game</h3>
                Here is a team project from another course this semester, Mutimedia Systems. Contributors: Haoran Yin,
                Nan Sai, and Yingjie Li.
                <div class="imagewrapper">
                    <video width="500" controls>
                        <source src="data/game.mp4" type="video/mp4">
                    </video>
                </div>
            </div>



            <div class="section">
                <h1>References</h1>
                <ol>
                    <li class="listitem" id="cite1">
                        Rothstein, J. (1992). MIDI: A comprehensive introduction (Vol. 7). AR Editions, Inc..
                    </li>
                    <li class="listitem" id="cite2">
                        Bittner, R. M., Bosch, J. J., Rubinstein, D., Meseguer-Brocal, G., & Ewert, S. (2022, May). A
                        Lightweight Instrument-Agnostic Model for Polyphonic Note Transcription and Multipitch
                        Estimation. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal
                        Processing (ICASSP) (pp. 781-785). IEEE.
                    </li>
                    <li class="listitem" id="cite3">
                        Wu, Y. T., Chen, B., & Su, L. (2020). Multi-instrument automatic music transcription with
                        self-attention-based instance segmentation. IEEE/ACM Transactions on Audio, Speech, and Language
                        Processing, 28, 2796-2809.
                    </li>
                    <li class="listitem" id="cite4">
                        Défossez, Alexandre, et al. Music source separation in the waveform domain. arXiv preprint
                        arXiv:1911.13254 (2019).
                    </li>
                </ol>
            </div>
        </main>
    </div>
</body>

</html>